{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edf5926-5c26-4403-a3a7-0b4a48884890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07047462-4a6c-49f8-803c-b5a7f71a9f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version : azure\n",
      "base url : https://litellm.toxpipe.niehs.nih.gov\n",
      "API_KEY : sk-dSTROKbqfoD8L2WDNDhwXA\n",
      "engin: azure-gpt-4o\n",
      "model_name: azure-gpt-4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "# Access environment variables\n",
    "api_key = os.getenv('API_KEY')\n",
    "api_type = os.getenv('OPENAI_API_TYPE')\n",
    "model_name = os.getenv('MODEL_NAME')\n",
    "\n",
    "base_url = os.getenv('BASE_URL')\n",
    "\n",
    "#DM_db_url = os.getenv('DRUGMATRIX_DB_URL')\n",
    "#database_url = os.getenv('DATABASE_URL')\n",
    "debug_mode = os.getenv('DEBUG')\n",
    "engin = os.getenv('MODEL_ENGINE')\n",
    "version = os.getenv('OPENAI_API_VERSION')\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_TYPE\"] = api_type\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://litellm.toxpipe.niehs.nih.gov\"\n",
    "os.environ[\"OPENAI_API_KEY\"] =  api_key\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = base_url\n",
    "os.environ[\"OPENAI_API_VERSION\"] = version \n",
    "\n",
    "print(f\"version : {api_type}\")\n",
    "print(f\"base url : {base_url}\")\n",
    "print(f\"API_KEY : {api_key}\")\n",
    "print(f\"engin: {engin}\")\n",
    "print(f\"model_name: {model_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65c5833c-6a1b-4c82-bd6d-c323a29fa184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from llama_index.multi_modal_llms.azure_openai import AzureOpenAIMultiModal\n",
    "from llama_index.legacy.multi_modal_llms.azure_openai import AzureOpenAIMultiModal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5186361c-6e9d-4dc7-9e89-b4787d672219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "from llama_index.core.schema import ImageDocument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d428348-fc72-412a-b30a-c2306a3da1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_openai_mm_llm = AzureOpenAIMultiModal(\n",
    "    model=\"gpt-4-vision-preview\",#\"gpt-4o\",\n",
    "    engine=engin,#\"azure-gpt-4o\",  # Replace with your deployment name\n",
    "    azure_endpoint=base_url,#azure_endpoint,  # Replace with your endpoint URL\n",
    "    api_key=api_key,  # Replace with your API key\n",
    "    api_version=version #\"2023-05-15\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed583e8a-639b-41f9-afdc-27e3dd8b5a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#file_path =  \"./jupyter_apps/LLM/imgs/ZF_LDTT.png\" \n",
    "file_path =  \"./imgs/ZF_LDTT.png\" \n",
    "# Open the image\n",
    "with open(file_path, \"rb\") as image_file:\n",
    "        image_data = image_file.read()\n",
    "base64str_ZF = base64.b64encode(image_data).decode(\"utf-8\")\n",
    "\n",
    "image_document_ZF = ImageDocument(image=base64str_ZF, image_mimetype=\"image/png\")\n",
    "\n",
    "#from PIL import Image        \n",
    "#image_data = Image.open(file_path)\n",
    "# Display the image (optional)\n",
    "#image.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e568773a-ff53-4c2d-b380-54a6c5586d14",
   "metadata": {},
   "outputs": [],
   "source": [
    " complete_response_ZF = azure_openai_mm_llm.complete(\n",
    "    prompt=\"Describe the images as an alternative text\",\n",
    "    image_documents=[image_document_ZF],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "984a3548-471c-4379-bb4e-4a893dc8abae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image is a database schema diagram created using dbdiagram.io. It consists of multiple tables with their respective fields and relationships. Here is a detailed description of each table and its fields:\n",
      "\n",
      "1. **sources**\n",
      "   - source_name (varchar)\n",
      "   - source_code (varchar)\n",
      "\n",
      "2. **chemicals**\n",
      "   - chemical_id (integer)\n",
      "   - chemical_name (varchar)\n",
      "   - chemical_code (varchar)\n",
      "   - casrn (varchar)\n",
      "   - preferred_name (varchar)\n",
      "\n",
      "3. **plates**\n",
      "   - plate_id (integer)\n",
      "   - plate_name (varchar)\n",
      "   - plate_type (varchar)\n",
      "   - plate_screen_time (timestamp)\n",
      "   - plate_stage (varchar)\n",
      "\n",
      "4. **well_types**\n",
      "   - well_type_id (integer)\n",
      "   - well_description (varchar)\n",
      "\n",
      "5. **devices_data**\n",
      "   - devices_data_id (integer)\n",
      "   - endpoint_id (integer)\n",
      "   - well_type (varchar)\n",
      "   - well_position (varchar)\n",
      "   - concentration_unit (varchar)\n",
      "   - vehicle (varchar)\n",
      "   - solvent (varchar)\n",
      "   - cell_density (varchar)\n",
      "   - stim_bladder_deflated (boolean)\n",
      "\n",
      "6. **behavior_data**\n",
      "   - devices_data_id (integer)\n",
      "   - endpoint_id (integer)\n",
      "   - distance_traveled (real)\n",
      "   - time_moving (real)\n",
      "\n",
      "7. **bins**\n",
      "   - bin_id (integer)\n",
      "   - bin_name (varchar)\n",
      "\n",
      "8. **phases**\n",
      "   - phase_id (integer)\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(complete_response_ZF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6879e384-58bd-40b5-8f36-04ffa7ca2b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe (question):\n",
    "    complete_response_ZF = azure_openai_mm_llm.complete(\n",
    "    #prompt=\"Describe the images as an alternative text\",\n",
    "    prompt=question,\n",
    "    image_documents=[image_document_ZF],\n",
    ")\n",
    "    return complete_response_ZF\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93d8105f-94d8-47f1-929a-5dc652bf5bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://6152bf3f578dfa787e.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://6152bf3f578dfa787e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr \n",
    "def greet(name): return \"Hello \" + name + \"!\" \n",
    "def prodiction (question): \n",
    "    answer = describe (question)\n",
    "    return answer\n",
    "demo = gr.Interface(fn=prodiction, inputs=\"textbox\", outputs=\"textbox\") \n",
    "if __name__ == \"__main__\": \n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44966db7-bd12-43e9-a98f-51d25e676d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/gradio/components/chatbot.py:222: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7875\n",
      "* Running on public URL: https://21676d4c39f6958bdc.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://21676d4c39f6958bdc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response type: <class 'llama_index.legacy.core.llms.types.CompletionResponse'>\n",
      "content :  The image is a database schema diagram created using dbdiagram.io. It consists of several tables with their respective columns and relationships. Here is a description of each table and its columns:\n",
      "\n",
      "1. **sources**\n",
      "   - source_name (varchar)\n",
      "   - source_id (integer, primary key)\n",
      "\n",
      "2. **chemicals**\n",
      "   - chemical_id (integer, primary key)\n",
      "   - chemical_code (varchar)\n",
      "   - casrn (varchar)\n",
      "   - preferred_name (varchar)\n",
      "\n",
      "3. **plates**\n",
      "   - plate_id (integer, primary key)\n",
      "   - plate_name (varchar)\n",
      "   - plate_type (varchar)\n",
      "   - plate_screen_time (timestamp)\n",
      "   - plate_stage (varchar)\n",
      "\n",
      "4. **well_types**\n",
      "   - well_type_id (integer, primary key)\n",
      "   - well_description (varchar)\n",
      "\n",
      "5. **devices_data**\n",
      "   - devices_data_id (integer, primary key)\n",
      "   - source_id (integer, foreign key)\n",
      "   - well_type (varchar)\n",
      "   - well_position (varchar)\n",
      "   - concentration_unit (varchar)\n",
      "   - vehicle (varchar)\n",
      "   - solvent (varchar)\n",
      "   - cell_line (varchar)\n",
      "   - cell_type (varchar)\n",
      "   - stim_bladder_defleeted (boolean)\n",
      "\n",
      "6. **behavior_data**\n",
      "   - devices_data_id (integer, foreign key)\n",
      "   - behavior_id (integer, primary key)\n",
      "   - time (timestamp)\n",
      "   - distance_moved (real)\n",
      "   - region (varchar)\n",
      "\n",
      "7. **bins**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from llama_index.legacy.multi_modal_llms.azure_openai import AzureOpenAIMultiModal\n",
    "import llama_index\n",
    "#import ollama\n",
    "\n",
    "def format_history(msg: str, history: list[list[str, str]], system_prompt: str):\n",
    "    chat_history = [{\"role\": \"system\", \"content\":system_prompt}]\n",
    "    for query, response in history:\n",
    "        chat_history.append({\"role\": \"user\", \"content\": query})\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": response})  \n",
    "    chat_history.append({\"role\": \"user\", \"content\": msg})\n",
    "    return chat_history\n",
    "\n",
    "def init_loadimages():\n",
    "     #file_path =  \"./jupyter_apps/LLM/imgs/ZF_LDTT.png\" \n",
    "    file_path =  \"./imgs/ZF_LDTT.png\" \n",
    "    \n",
    "    try:\n",
    "        # Read the uploaded image file\n",
    "        with open(file_path, \"rb\") as img_file:\n",
    "            image_data = img_file.read()\n",
    "        \n",
    "        # Convert image to base64 and create ImageDocument\n",
    "        base64str = base64.b64encode(image_data).decode(\"utf-8\")\n",
    "        image_document = ImageDocument(image=base64str, image_mimetype=\"image/png\")\n",
    "        return image_document\n",
    "         \n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "    \n",
    "def generate_response(msg: str, history: list[list[str, str]], system_prompt: str):\n",
    "    chat_history = format_history(msg, history, system_prompt)\n",
    "    image_document = init_loadimages()\n",
    "    response = azure_openai_mm_llm.complete(\n",
    "        prompt=\"Describe the images as an alternative text\",\n",
    "        image_documents=[image_document],\n",
    "    )\n",
    "      # Check response type and print it\n",
    "    print(\"Response type:\", type(response))\n",
    "    #print(\"Response :\", response)\n",
    "    \n",
    "    #from llama_index.core import Response\n",
    "    #text = chat_response.message.content\n",
    "    #response = Response(text)\n",
    "    \n",
    "    # Handle CompletionResponse\n",
    "    if isinstance(response, llama_index.legacy.core.llms.types.CompletionResponse):\n",
    "        try:\n",
    "            content = response.text  # Assuming the content is in the \"text\" attribute\n",
    "            #content = complete_response_ZF.text\n",
    "            #content = data.get('content')\n",
    "            #print(\"content type : \",type(content))\n",
    "            print(\"content : \",content)\n",
    "            max_length = 200  # Adjust this value as needed\n",
    "            shortened_content = content[:max_length] + \"...\"\n",
    "            #yield [(\"assistant\", shortened_content)]             \n",
    "            yield [{\"role\": \"system\", \"content\":shortened_content}]\n",
    "            \n",
    "            #table_descriptions = shortened_content.split(\"\\n\\n\")  # Split by double newline\n",
    "            #for description in table_descriptions:\n",
    "                #yield [(\"description\",description)]\n",
    "            #yield [(\"assistant\",  \"my test content\")]\n",
    "        except AttributeError:\n",
    "            # Handle potential absence of \"text\" attribute\n",
    "            yield {\"assistant\",  \"Error: Unexpected response format 1\"}\n",
    "        \n",
    "    else:\n",
    "        # Handle other unexpected formats (if needed)\n",
    "        yield {\"assistant\",  \"Error: Unexpected response format 2\"}\n",
    "    \n",
    "    \n",
    "     \n",
    "    #message = response\n",
    "    #print (response)\n",
    "    #yield message\n",
    "#gr.Markdown (\"## kskssksksk\")\n",
    "chatbot = gr.ChatInterface(\n",
    "                \n",
    "                generate_response,\n",
    "                chatbot=gr.Chatbot(\n",
    "                        avatar_images=[\"./imgs/user.png\", \"./imgs/Chatbot.jpg\"],\n",
    "                        height=\"64vh\"\n",
    "                    ),\n",
    "                additional_inputs=[\n",
    "                    gr.Textbox(\n",
    "                        \"Behave as if you are professional datascientist.\",\n",
    "                        label=\"System Prompt\"\n",
    "                    )\n",
    "                ],\n",
    "                \n",
    "    \n",
    "                title=\"🦜🔗 LLama Chatbot database images\",\n",
    "                description=\"Feel free to ask any question about the zebra file database.\",\n",
    "                theme=\"soft\",\n",
    "                submit_btn=\"⬅ Send\",\n",
    "                #retry_btn=\"🔄 Regenerate Response\",\n",
    "                #undo_btn=\"↩ Delete Previous\",\n",
    "                #clear_btn=\"🗑️ Clear Chat\"\n",
    ")\n",
    "chatbot.launch( share=True)\n",
    "#chatbot.launch(server_name='https://jupyter-gpu.niehs.nih.gov/', server_port=7860)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "176cb56d-3e73-4c89-91ce-fbb15e09aeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image is a database schema diagram created using dbdiagram.io. It consists of several tables with their respective columns and relationships. The tables and their columns are as follows:\n",
      "\n",
      "1. **sources**\n",
      "   - source_name (varchar)\n",
      "   - source_id (integer)\n",
      "\n",
      "2. **chemicals**\n",
      "   - chemical_id (integer)\n",
      "   - chemical_name (varchar)\n",
      "   - chemical_code (varchar)\n",
      "   - casrn (varchar)\n",
      "   - preferred_name (varchar)\n",
      "\n",
      "3. **plates**\n",
      "   - plate_id (integer)\n",
      "   - plate_name (varchar)\n",
      "   - plate_type (varchar)\n",
      "   - plate_screen_time (integer)\n",
      "   - plate_screen_stage (varchar)\n",
      "\n",
      "4. **well_types**\n",
      "   - well_type_id (integer)\n",
      "   - well_description (varchar)\n",
      "\n",
      "5. **devices_data**\n",
      "   - devices_data_id (integer)\n",
      "   - source_id (integer)\n",
      "   - device_id (integer)\n",
      "   - well_type (varchar)\n",
      "   - well_position (varchar)\n",
      "   - concentration_unit (varchar)\n",
      "   - vehicle (varchar)\n",
      "   - solvent (varchar)\n",
      "   - cell_line (varchar)\n",
      "   - cell_density (varchar)\n",
      "   - stim_bladder_deflated (boolean)\n",
      "\n",
      "6. **bins**\n",
      "   - bin_id (integer)\n",
      "   - bin_name (varchar)\n",
      "\n",
      "7. **phases**\n",
      "   - phase_id (integer)\n",
      "   - phase_name (varchar)\n",
      "   - phase_stage (varchar)\n",
      "\n",
      "8. **endpoints**\n",
      "   - endpoint_id (integer\n"
     ]
    }
   ],
   "source": [
    "#print(response.text) \n",
    "print(complete_response_ZF.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c77aa49b-3c98-4327-9938-f70a2e7ad048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/gradio/components/chatbot.py:222: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7880\n",
      "* Running on public URL: https://93f04755c61d4af684.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://93f04755c61d4af684.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "#import ollama\n",
    "\n",
    "def format_history(msg: str, history: list[list[str, str]], system_prompt: str):\n",
    "    chat_history = [{\"role\": \"system\", \"content\":system_prompt}]\n",
    "    for query, response in history:\n",
    "        chat_history.append({\"role\": \"user\", \"content\": query})\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": response})  \n",
    "    chat_history.append({\"role\": \"user\", \"content\": msg})\n",
    "    return chat_history\n",
    "\n",
    "def init_loadimages():\n",
    "     #file_path =  \"./jupyter_apps/LLM/imgs/ZF_LDTT.png\" \n",
    "    file_path =  \"./imgs/ZF_LDTT.png\" \n",
    "    \n",
    "    try:\n",
    "        # Read the uploaded image file\n",
    "        with open(file_path, \"rb\") as img_file:\n",
    "            image_data = img_file.read()\n",
    "        \n",
    "        # Convert image to base64 and create ImageDocument\n",
    "        base64str = base64.b64encode(image_data).decode(\"utf-8\")\n",
    "        image_document = ImageDocument(image=base64str, image_mimetype=\"image/png\")\n",
    "        return image_document\n",
    "         \n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "    \n",
    "def generate_response(msg: str, history: list[list[str, str]], system_prompt: str):\n",
    "    chat_history = format_history(msg, history, system_prompt)\n",
    "    image_document = init_loadimages()\n",
    "    response = azure_openai_mm_llm.complete(\n",
    "        prompt=\"Describe the images as an alternative text\",\n",
    "        image_documents=[image_document],\n",
    "    )\n",
    "      # Check response type and print it\n",
    "    print(\"Response type:\", type(response))\n",
    "    \n",
    "    # Handle CompletionResponse\n",
    "    if isinstance(response, llama_index.legacy.core.llms.types.CompletionResponse):\n",
    "        try:\n",
    "            content = response.text  # Assuming the content is in the \"text\" attribute\n",
    "            content = \"\"\"{ 'role': 'assistant',\n",
    "    'content': 'The image is a database schema diagram with several tables, including sources, chemicals, plates, well_types}\"\"\" \n",
    "            content =\"\"\"{'chat_message':'hello'}\"\"\"\n",
    "            \n",
    "            content = {'role': 'assistant','content': 'HHHHHHHHello, how can I assist you today?'}\n",
    "            \n",
    "\n",
    "            yield {\"role\": \"assistant\", \"content\": content}\n",
    "        except AttributeError:\n",
    "            # Handle potential absence of \"text\" attribute\n",
    "            yield {\"role\": \"assistant\", \"content\": \"Error: Unexpected response format\"}\n",
    "    else:\n",
    "        # Handle other unexpected formats (if needed)\n",
    "        yield {\"role\": \"assistant\", \"content\": \"Error: Unexpected response format\"}\n",
    "    \n",
    "    \n",
    "    # Check if response is a string (assuming it's the desired format)\n",
    "    if isinstance(response, str):\n",
    "        # Yield the complete response as a single message\n",
    "        yield {\"role\": \"assistant\", \"content\": response}\n",
    "        \n",
    "    else:\n",
    "        if isinstance(response, list):\n",
    "          # Handle list format (assuming first element is content)\n",
    "            try:\n",
    "                content = response[0]  # Assuming content is at index 0 (adjust if needed)\n",
    "                content = {'role': 'assistant','content': 'Hello, how can I assist you today?'}\n",
    "                if content:\n",
    "                    yield {\"role\": \"assistant\", \"content\": content}\n",
    "            except IndexError:\n",
    "                # Handle potential issues with accessing list elements\n",
    "                yield {\"role\": \"assistant\", \"content\": \"Error: Unexpected response format\"}\n",
    "        else:\n",
    "            # Handle other unexpected formats (if needed)\n",
    "            yield {\"role\": \"assistant\", \"content\": \"Error: Unexpected response format 1 \"}\n",
    "        # Handle other response formats (if necessary)\n",
    "        yield {\"role\": \"assistant\", \"content\": \"Error: Unexpected response format 2 \"}\n",
    "     \n",
    "    #message = response\n",
    "    #print (response)\n",
    "    #yield message\n",
    "\n",
    "chatbot = gr.ChatInterface(\n",
    "                generate_response,\n",
    "                chatbot=gr.Chatbot(\n",
    "                        avatar_images=[\"./imgs/user.png\", \"./imgs/Chatbot.jpeg\"],\n",
    "                        height=\"64vh\"\n",
    "                    ),\n",
    "                additional_inputs=[\n",
    "                    gr.Textbox(\n",
    "                        \"Behave as if you are professional writer.\",\n",
    "                        label=\"System Prompt\"\n",
    "                    )\n",
    "                ],\n",
    "                title=\"LLama-2 (7B) Chatbot using 'Ollama'\",\n",
    "                description=\"Feel free to ask any question.\",\n",
    "                theme=\"soft\",\n",
    "                submit_btn=\"⬅ Send\",\n",
    "                #retry_btn=\"🔄 Regenerate Response\",\n",
    "                #undo_btn=\"↩ Delete Previous\",\n",
    "                #clear_btn=\"🗑️ Clear Chat\"\n",
    ")\n",
    "chatbot.launch( share=True)\n",
    "#chatbot.launch(server_name='https://jupyter-gpu.niehs.nih.gov/', server_port=7860)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df859762-412b-4c9b-bce9-68345fade26f",
   "metadata": {},
   "source": [
    "## Chatbot for the sql images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53fa8c31-1857-40d8-a14d-f04362cb7fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/gradio/components/chatbot.py:222: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7871\n",
      "* Running on public URL: https://4889a6d9b344eb88df.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://4889a6d9b344eb88df.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/gradio/queueing.py\", line 622, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/gradio/blocks.py\", line 2013, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/gradio/blocks.py\", line 1578, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/gradio/utils.py\", line 691, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/gradio/utils.py\", line 796, in asyncgen_wrapper\n",
      "    response = await iterator.__anext__()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/gradio/chat_interface.py\", line 666, in _stream_fn\n",
      "    first_response = await async_iteration(generator)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/gradio/utils.py\", line 691, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/gradio/utils.py\", line 685, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2405, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 914, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/gradio/utils.py\", line 668, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2239099/4079005690.py\", line 42, in generate_response\n",
      "    if isinstance(partial_resp[\"message\"], dict):\n",
      "                  ~~~~~~~~~~~~^^^^^^^^^^^\n",
      "TypeError: tuple indices must be integers or slices, not str\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "#import ollama\n",
    "\n",
    "def format_history(msg: str, history: list[list[str, str]], system_prompt: str):\n",
    "    chat_history = [{\"role\": \"system\", \"content\":system_prompt}]\n",
    "    for query, response in history:\n",
    "        chat_history.append({\"role\": \"user\", \"content\": query})\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": response})  \n",
    "    chat_history.append({\"role\": \"user\", \"content\": msg})\n",
    "    return chat_history\n",
    "\n",
    "def init_loadimages():\n",
    "     #file_path =  \"./jupyter_apps/LLM/imgs/ZF_LDTT.png\" \n",
    "    file_path =  \"./imgs/ZF_LDTT.png\" \n",
    "    \n",
    "    try:\n",
    "        # Read the uploaded image file\n",
    "        with open(file_path, \"rb\") as img_file:\n",
    "            image_data = img_file.read()\n",
    "        \n",
    "        # Convert image to base64 and create ImageDocument\n",
    "        base64str = base64.b64encode(image_data).decode(\"utf-8\")\n",
    "        image_document = ImageDocument(image=base64str, image_mimetype=\"image/png\")\n",
    "        return image_document\n",
    "         \n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "    \n",
    "def generate_response(msg: str, history: list[list[str, str]], system_prompt: str):\n",
    "    chat_history = format_history(msg, history, system_prompt)\n",
    "    image_document = init_loadimages()\n",
    "    response = azure_openai_mm_llm.complete(\n",
    "        prompt=\"Describe the images as an alternative text\",\n",
    "        image_documents=[image_document],\n",
    "    )\n",
    "    message = response\n",
    "    #print (response)\n",
    "    for partial_resp in response:\n",
    "        try:\n",
    "            # Check if \"message\" is a dictionary and access content\n",
    "            if isinstance(partial_resp[\"message\"], dict):\n",
    "                token = partial_resp[\"message\"][\"content\"]\n",
    "            # Handle list format (example)\n",
    "            elif isinstance(partial_resp[\"message\"], list):\n",
    "                for message_item in partial_resp[\"message\"]:\n",
    "                    content = message_item.get(\"content\", None)\n",
    "                    if content:\n",
    "                        message += content\n",
    "                        break  # Stop iterating after finding content (optional)\n",
    "            elif isinstance(partial_resp[\"message\", tuple]):\n",
    "                content = partial_resp[\"message\"][0]\n",
    "                if content:\n",
    "                  message += content\n",
    "            else:\n",
    "                # Handle other formats if needed\n",
    "                pass\n",
    "            message += token\n",
    "        except KeyError:\n",
    "            # Handle potential absence of \"message\" key\n",
    "            message += \"Error: Could not access response content.\"\n",
    "        yield message\n",
    "\n",
    "chatbot = gr.ChatInterface(\n",
    "                generate_response,\n",
    "                chatbot=gr.Chatbot(\n",
    "                        avatar_images=[\"./imgs/user.png\", \"./imgs/Chatbot.jpg\"],\n",
    "                        height=\"64vh\"\n",
    "                    ),\n",
    "                additional_inputs=[\n",
    "                    gr.Textbox(\n",
    "                        \"Behave as if you are professional writer.\",\n",
    "                        label=\"System Prompt\"\n",
    "                    )\n",
    "                ],\n",
    "                title=\"LLama-2 (7B) Chatbot using 'Ollama'\",\n",
    "                description=\"Feel free to ask any question.\",\n",
    "                theme=\"soft\",\n",
    "                submit_btn=\"⬅ Send\",\n",
    "                #retry_btn=\"🔄 Regenerate Response\",\n",
    "                #undo_btn=\"↩ Delete Previous\",\n",
    "                #clear_btn=\"🗑️ Clear Chat\"\n",
    ")\n",
    "chatbot.launch( share=True)\n",
    "#chatbot.launch(server_name='https://jupyter-gpu.niehs.nih.gov/', server_port=7860)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39cf2e1-bf5e-4f1f-b66e-a1d9400c4de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c80f9a-0652-4bf8-b94b-7d1d696fa75d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

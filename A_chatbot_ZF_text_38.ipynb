{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1edf5926-5c26-4403-a3a7-0b4a48884890",
   "metadata": {},
   "outputs": [],
   "source": [
    "## From a text schema file based on user's question provide information about the database and also provide the SQL  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07047462-4a6c-49f8-803c-b5a7f71a9f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version : azure\n",
      "azure_endpoint : https://litellm.toxpipe.niehs.nih.gov\n",
      "API_KEY : sk-dSTROKbqfoD8L2WDNDhwXA\n",
      "engin: azure-gpt-4o\n",
      "model_name: azure-gpt-4\n",
      "API version: 2023-05-15\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "# Access environment variables\n",
    "api_key = os.getenv('API_KEY')\n",
    "api_type = os.getenv('OPENAI_API_TYPE')\n",
    "model_name = os.getenv('MODEL_NAME')\n",
    "\n",
    "#base_url = os.getenv('BASE_URL')\n",
    "azure_endpoint =  os.getenv('BASE_URL')\n",
    "#DM_db_url = os.getenv('DRUGMATRIX_DB_URL')\n",
    "#database_url = os.getenv('DATABASE_URL')\n",
    "debug_mode = os.getenv('DEBUG')\n",
    "engin = os.getenv('MODEL_ENGINE')\n",
    "version = os.getenv('OPENAI_API_VERSION')\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_TYPE\"] = api_type\n",
    "os.environ[\"OPENAI_API_BASE\"] = azure_endpoint#\"https://litellm.toxpipe.niehs.nih.gov\"\n",
    "os.environ[\"OPENAI_API_KEY\"] =  api_key\n",
    "#os.environ[\"AZURE_OPENAI_ENDPOINT\"] = base_url\n",
    "os.environ[\"OPENAI_API_VERSION\"] = version \n",
    "\n",
    "print(f\"version : {api_type}\")\n",
    "#print(f\"base url : {base_url}\")\n",
    "print(f\"azure_endpoint : {azure_endpoint}\")\n",
    "print(f\"API_KEY : {api_key}\")\n",
    "print(f\"engin: {engin}\")\n",
    "print(f\"model_name: {model_name}\")\n",
    "print(f\"API version: {version}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65c5833c-6a1b-4c82-bd6d-c323a29fa184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from llama_index.multi_modal_llms.azure_openai import AzureOpenAIMultiModal\n",
    "from llama_index.legacy.multi_modal_llms.azure_openai import AzureOpenAIMultiModal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5186361c-6e9d-4dc7-9e89-b4787d672219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "from llama_index.core.schema import ImageDocument\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e76a95e-0dbd-4f07-9de9-a44ec94d53f3",
   "metadata": {},
   "source": [
    "azure_openai_mm_llm = AzureOpenAIMultiModal(\n",
    "    model=\"gpt-4-vision-preview\",#\"gpt-4o\",\n",
    "    engine=engin,#\"azure-gpt-4o\",  # Replace with your deployment name\n",
    "    azure_endpoint=base_url,#azure_endpoint,  # Replace with your endpoint URL\n",
    "    api_key=api_key,  # Replace with your API key\n",
    "    api_version=version #\"2023-05-15\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9a6d0fc-b703-4dda-b4e3-fcc7eebffb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(temperature=0.1, model=model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d665bca-fd39-4145-9803-6d04049f3bea",
   "metadata": {},
   "source": [
    "## Read in the text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a10e26a-7833-4caf-9218-bf25df9a6da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices import MultiModalVectorStoreIndex\n",
    "#from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.legacy.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import SimpleDirectoryReader, StorageContext\n",
    "# \"`qdrant-client` package not found, please run `pip install qdrant-client`\"\n",
    "#pip install qdrant-client\n",
    "import qdrant_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f855719f-b59b-44c1-a46e-41fa06cf7bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"./sql_files\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49446c8b-c0da-4584-9864-d304eb18d14e",
   "metadata": {},
   "source": [
    "# Create a local Qdrant vector store\n",
    "#file_path=\"./sql_files\"\n",
    "client = qdrant_client.QdrantClient(path=file_path)\n",
    "\n",
    "text_store = QdrantVectorStore(client=client, collection_name=\"text_collection\")\n",
    "image_store = QdrantVectorStore(client=client, collection_name=\"image_collection\")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=text_store, image_store=image_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b280445f-7c51-4250-8d0f-1a5ef22727f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "#documents = SimpleDirectoryReader(\"./data_drugmatrix/\").load_data()\n",
    "documents = SimpleDirectoryReader(file_path).load_data()\n",
    "print(len(documents))\n",
    "from llama_index.core import Document, VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2981853-f811-43ac-83a8-183176d85eeb",
   "metadata": {},
   "source": [
    "# Prepare the completion request\n",
    "response = llm.completions.create(\n",
    "    engine=\"gpt-4\",  # or \"gpt-3.5-turbo\"\n",
    "    prompt=f\"Summarize the following text: \\n {index}\",\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "# Print the response text\n",
    "print(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b7c30af-fe81-4e41-8390-11f3cf4930fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2378033/2505712151.py:6: LangChainDeprecationWarning: The class `AzureOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import AzureOpenAI`.\n",
      "  azure_llm = AzureOpenAI(\n",
      "/tmp/ipykernel_2378033/2505712151.py:6: UserWarning: WARNING! engine is not default parameter.\n",
      "                engine was transferred to model_kwargs.\n",
      "                Please confirm that engine is what you intended.\n",
      "  azure_llm = AzureOpenAI(\n",
      "/ddn/gs1/home/noltesz/.conda/envs/py38/lib/python3.8/site-packages/langchain_community/llms/openai.py:883: UserWarning: As of openai>=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from https://litellm.toxpipe.niehs.nih.gov to https://litellm.toxpipe.niehs.nih.gov/openai.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#from llama_index.llms import AzureOpenAI\n",
    "from langchain_community.llms.openai import AzureOpenAI\n",
    "#from llama_index.embeddings import AzureOpenAIEmbedding\n",
    "#from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "\n",
    "azure_llm = AzureOpenAI(\n",
    "    engine=\"azure-gpt-4o\", # model_name on litellm proxy\n",
    "    temperature=0.0,\n",
    "    #azure_endpoint = azure_endpoint,\n",
    "    #base_url=\"https://litellm.toxpipe.niehs.nih.gov\", # litellm proxy endpoint\n",
    "    api_key=api_key, # litellm proxy API Key\n",
    "    api_version=version,#\"2024-02-01\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02ceca62-ca05-4e22-a615-87f4cc1794da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-AbAXlD58Z2TvTYInAtyuXHRPIbIdh', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The provided document contains SQL schema definitions for a database related to devices, chemicals, and their interactions in various experimental setups. The schema includes the following tables:\\n\\n1. **devices_data:** Stores information about different devices, including details like device ID, name, well type, position, concentration unit, and more.\\n2. **endpoints:** Contains endpoint IDs and their descriptive information.\\n3. **endpoint_data:** Records endpoint values linked to specific devices.\\n4. **sources:** Holds information about various sources, each identified by a unique source ID and code.\\n5. **chemicals:** Details different chemicals, including attributes like chemical code, DTXSID, CASRN, and preferred name.\\n6. **well_types:** Describes types of wells used in experiments.\\n7. **plates:** Represents plates used in experiments, including details like plate ID, name, screening time, and stage.\\n8. **bins:** Contains bin information, likely for categorizing data ranges.\\n9. **phases:** Details different phases of experiments, linked to sources via source code.\\n10. **phase_bins:** Maps phases to bins.\\n11. **devtox_data:** Stores developmental toxicity data, including plate and chemical IDs, well information, concentration, and experimental outcomes (e.g., dead/malformed statuses).\\n12. **behavior_data:** Records behavioral data associated with developmental toxicity results, such as distance moved and time.\\n\\nAdditionally, a metadata file in JSON format is included, which contains minimal content indicating collections and aliases but no specific data.', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1733422029, model='gpt-4o-2024-05-13', object='chat.completion', service_tier=None, system_fingerprint='fp_04751d0b65', usage=CompletionUsage(completion_tokens=305, prompt_tokens=1307, total_tokens=1612, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=api_key, # Format should be 'sk-<your_key>'\n",
    "    base_url=azure_endpoint #\"http://litellm.toxpipe.niehs.nih.gov\" # LiteLLM Proxy is OpenAI compatible, Read More: https://docs.litellm.ai/docs/proxy/user_keys\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"azure-gpt-4o\", # model to send to the proxy\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            #\"content\": \"this is a test request, write a short poem\"\n",
    "             \"content\": f\"Summarize the following text:\\n {documents}\",\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f6d425-848a-422c-9f44-71ac3ba0ea2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dd96976-0021-421b-8bbc-f05e3dc03fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided document contains SQL schema definitions for a database related to devices, chemicals, and their interactions in various experimental setups. The schema includes the following tables:\n",
      "\n",
      "1. **devices_data:** Stores information about different devices, including details like device ID, name, well type, position, concentration unit, and more.\n",
      "2. **endpoints:** Contains endpoint IDs and their descriptive information.\n",
      "3. **endpoint_data:** Records endpoint values linked to specific devices.\n",
      "4. **sources:** Holds information about various sources, each identified by a unique source ID and code.\n",
      "5. **chemicals:** Details different chemicals, including attributes like chemical code, DTXSID, CASRN, and preferred name.\n",
      "6. **well_types:** Describes types of wells used in experiments.\n",
      "7. **plates:** Represents plates used in experiments, including details like plate ID, name, screening time, and stage.\n",
      "8. **bins:** Contains bin information, likely for categorizing data ranges.\n",
      "9. **phases:** Details different phases of experiments, linked to sources via source code.\n",
      "10. **phase_bins:** Maps phases to bins.\n",
      "11. **devtox_data:** Stores developmental toxicity data, including plate and chemical IDs, well information, concentration, and experimental outcomes (e.g., dead/malformed statuses).\n",
      "12. **behavior_data:** Records behavioral data associated with developmental toxicity results, such as distance moved and time.\n",
      "\n",
      "Additionally, a metadata file in JSON format is included, which contains minimal content indicating collections and aliases but no specific data.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d01a39-c65c-491b-bc49-e3c762f36cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e46f78d-6cc1-43f6-a39e-f095192a09c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(complete_response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6879e384-58bd-40b5-8f36-04ffa7ca2b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-AbAXlD58Z2TvTYInAtyuXHRPIbIdh', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The provided document contains SQL schema definitions for a database related to devices, chemicals, and their interactions in various experimental setups. The schema includes the following tables:\\n\\n1. **devices_data:** Stores information about different devices, including details like device ID, name, well type, position, concentration unit, and more.\\n2. **endpoints:** Contains endpoint IDs and their descriptive information.\\n3. **endpoint_data:** Records endpoint values linked to specific devices.\\n4. **sources:** Holds information about various sources, each identified by a unique source ID and code.\\n5. **chemicals:** Details different chemicals, including attributes like chemical code, DTXSID, CASRN, and preferred name.\\n6. **well_types:** Describes types of wells used in experiments.\\n7. **plates:** Represents plates used in experiments, including details like plate ID, name, screening time, and stage.\\n8. **bins:** Contains bin information, likely for categorizing data ranges.\\n9. **phases:** Details different phases of experiments, linked to sources via source code.\\n10. **phase_bins:** Maps phases to bins.\\n11. **devtox_data:** Stores developmental toxicity data, including plate and chemical IDs, well information, concentration, and experimental outcomes (e.g., dead/malformed statuses).\\n12. **behavior_data:** Records behavioral data associated with developmental toxicity results, such as distance moved and time.\\n\\nAdditionally, a metadata file in JSON format is included, which contains minimal content indicating collections and aliases but no specific data.', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1733422029, model='gpt-4o-2024-05-13', object='chat.completion', service_tier=None, system_fingerprint='fp_04751d0b65', usage=CompletionUsage(completion_tokens=305, prompt_tokens=1307, total_tokens=1612, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "def describe (question):\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"azure-gpt-4o\", # model to send to the proxy\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            #\"content\": \"this is a test request, write a short poem\"\n",
    "             \"content\": f\"{question}:\\n {documents}\",\n",
    "             #\"content\": f\"Summarize the following text:\\n {documents}\",\n",
    "            \n",
    "        }\n",
    "    ]\n",
    ")\n",
    "    return response.choices[0].message.content\n",
    "print(response)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93d8105f-94d8-47f1-929a-5dc652bf5bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ddn/gs1/home/noltesz/.conda/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://ac496b6601b9ae2c3f.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ac496b6601b9ae2c3f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr \n",
    "def greet(name): return \"Hello \" + name + \"!\" \n",
    "def prodiction (question): \n",
    "    answer = describe (question)\n",
    "    return answer\n",
    "\n",
    "demo = gr.Interface(fn=prodiction, inputs=\"textbox\", outputs=\"textbox\") \n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44966db7-bd12-43e9-a98f-51d25e676d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/gradio/components/chatbot.py:222: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7875\n",
      "* Running on public URL: https://21676d4c39f6958bdc.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://21676d4c39f6958bdc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response type: <class 'llama_index.legacy.core.llms.types.CompletionResponse'>\n",
      "content :  The image is a database schema diagram created using dbdiagram.io. It consists of several tables with their respective columns and relationships. Here is a description of each table and its columns:\n",
      "\n",
      "1. **sources**\n",
      "   - source_name (varchar)\n",
      "   - source_id (integer, primary key)\n",
      "\n",
      "2. **chemicals**\n",
      "   - chemical_id (integer, primary key)\n",
      "   - chemical_code (varchar)\n",
      "   - casrn (varchar)\n",
      "   - preferred_name (varchar)\n",
      "\n",
      "3. **plates**\n",
      "   - plate_id (integer, primary key)\n",
      "   - plate_name (varchar)\n",
      "   - plate_type (varchar)\n",
      "   - plate_screen_time (timestamp)\n",
      "   - plate_stage (varchar)\n",
      "\n",
      "4. **well_types**\n",
      "   - well_type_id (integer, primary key)\n",
      "   - well_description (varchar)\n",
      "\n",
      "5. **devices_data**\n",
      "   - devices_data_id (integer, primary key)\n",
      "   - source_id (integer, foreign key)\n",
      "   - well_type (varchar)\n",
      "   - well_position (varchar)\n",
      "   - concentration_unit (varchar)\n",
      "   - vehicle (varchar)\n",
      "   - solvent (varchar)\n",
      "   - cell_line (varchar)\n",
      "   - cell_type (varchar)\n",
      "   - stim_bladder_defleeted (boolean)\n",
      "\n",
      "6. **behavior_data**\n",
      "   - devices_data_id (integer, foreign key)\n",
      "   - behavior_id (integer, primary key)\n",
      "   - time (timestamp)\n",
      "   - distance_moved (real)\n",
      "   - region (varchar)\n",
      "\n",
      "7. **bins**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from llama_index.legacy.multi_modal_llms.azure_openai import AzureOpenAIMultiModal\n",
    "import llama_index\n",
    "#import ollama\n",
    "\n",
    "def format_history(msg: str, history: list[list[str, str]], system_prompt: str):\n",
    "    chat_history = [{\"role\": \"system\", \"content\":system_prompt}]\n",
    "    for query, response in history:\n",
    "        chat_history.append({\"role\": \"user\", \"content\": query})\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": response})  \n",
    "    chat_history.append({\"role\": \"user\", \"content\": msg})\n",
    "    return chat_history\n",
    "\n",
    "def init_loadimages():\n",
    "     #file_path =  \"./jupyter_apps/LLM/imgs/ZF_LDTT.png\" \n",
    "    file_path =  \"./imgs/ZF_LDTT.png\" \n",
    "    \n",
    "    try:\n",
    "        # Read the uploaded image file\n",
    "        with open(file_path, \"rb\") as img_file:\n",
    "            image_data = img_file.read()\n",
    "        \n",
    "        # Convert image to base64 and create ImageDocument\n",
    "        base64str = base64.b64encode(image_data).decode(\"utf-8\")\n",
    "        image_document = ImageDocument(image=base64str, image_mimetype=\"image/png\")\n",
    "        return image_document\n",
    "         \n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "    \n",
    "def generate_response(msg: str, history: list[list[str, str]], system_prompt: str):\n",
    "    chat_history = format_history(msg, history, system_prompt)\n",
    "    image_document = init_loadimages()\n",
    "    response = azure_openai_mm_llm.complete(\n",
    "        prompt=\"Describe the images as an alternative text\",\n",
    "        image_documents=[image_document],\n",
    "    )\n",
    "      # Check response type and print it\n",
    "    print(\"Response type:\", type(response))\n",
    "    #print(\"Response :\", response)\n",
    "    \n",
    "    #from llama_index.core import Response\n",
    "    #text = chat_response.message.content\n",
    "    #response = Response(text)\n",
    "    \n",
    "    # Handle CompletionResponse\n",
    "    if isinstance(response, llama_index.legacy.core.llms.types.CompletionResponse):\n",
    "        try:\n",
    "            content = response.text  # Assuming the content is in the \"text\" attribute\n",
    "            #content = complete_response_ZF.text\n",
    "            #content = data.get('content')\n",
    "            #print(\"content type : \",type(content))\n",
    "            print(\"content : \",content)\n",
    "            max_length = 200  # Adjust this value as needed\n",
    "            shortened_content = content[:max_length] + \"...\"\n",
    "            #yield [(\"assistant\", shortened_content)]             \n",
    "            yield [{\"role\": \"system\", \"content\":shortened_content}]\n",
    "            \n",
    "            #table_descriptions = shortened_content.split(\"\\n\\n\")  # Split by double newline\n",
    "            #for description in table_descriptions:\n",
    "                #yield [(\"description\",description)]\n",
    "            #yield [(\"assistant\",  \"my test content\")]\n",
    "        except AttributeError:\n",
    "            # Handle potential absence of \"text\" attribute\n",
    "            yield {\"assistant\",  \"Error: Unexpected response format 1\"}\n",
    "        \n",
    "    else:\n",
    "        # Handle other unexpected formats (if needed)\n",
    "        yield {\"assistant\",  \"Error: Unexpected response format 2\"}\n",
    "    \n",
    "    \n",
    "     \n",
    "    #message = response\n",
    "    #print (response)\n",
    "    #yield message\n",
    "#gr.Markdown (\"## kskssksksk\")\n",
    "chatbot = gr.ChatInterface(\n",
    "                \n",
    "                generate_response,\n",
    "                chatbot=gr.Chatbot(\n",
    "                        avatar_images=[\"./imgs/user.png\", \"./imgs/Chatbot.jpg\"],\n",
    "                        height=\"64vh\"\n",
    "                    ),\n",
    "                additional_inputs=[\n",
    "                    gr.Textbox(\n",
    "                        \"Behave as if you are professional datascientist.\",\n",
    "                        label=\"System Prompt\"\n",
    "                    )\n",
    "                ],\n",
    "                \n",
    "    \n",
    "                title=\"🦜🔗 LLama Chatbot database images\",\n",
    "                description=\"Feel free to ask any question about the zebra file database.\",\n",
    "                theme=\"soft\",\n",
    "                submit_btn=\"⬅ Send\",\n",
    "                #retry_btn=\"🔄 Regenerate Response\",\n",
    "                #undo_btn=\"↩ Delete Previous\",\n",
    "                #clear_btn=\"🗑️ Clear Chat\"\n",
    ")\n",
    "chatbot.launch( share=True)\n",
    "#chatbot.launch(server_name='https://jupyter-gpu.niehs.nih.gov/', server_port=7860)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "176cb56d-3e73-4c89-91ce-fbb15e09aeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image is a database schema diagram created using dbdiagram.io. It consists of several tables with their respective columns and relationships. The tables and their columns are as follows:\n",
      "\n",
      "1. **sources**\n",
      "   - source_name (varchar)\n",
      "   - source_id (integer)\n",
      "\n",
      "2. **chemicals**\n",
      "   - chemical_id (integer)\n",
      "   - chemical_name (varchar)\n",
      "   - chemical_code (varchar)\n",
      "   - casrn (varchar)\n",
      "   - preferred_name (varchar)\n",
      "\n",
      "3. **plates**\n",
      "   - plate_id (integer)\n",
      "   - plate_name (varchar)\n",
      "   - plate_type (varchar)\n",
      "   - plate_screen_time (integer)\n",
      "   - plate_screen_stage (varchar)\n",
      "\n",
      "4. **well_types**\n",
      "   - well_type_id (integer)\n",
      "   - well_description (varchar)\n",
      "\n",
      "5. **devices_data**\n",
      "   - devices_data_id (integer)\n",
      "   - source_id (integer)\n",
      "   - device_id (integer)\n",
      "   - well_type (varchar)\n",
      "   - well_position (varchar)\n",
      "   - concentration_unit (varchar)\n",
      "   - vehicle (varchar)\n",
      "   - solvent (varchar)\n",
      "   - cell_line (varchar)\n",
      "   - cell_density (varchar)\n",
      "   - stim_bladder_deflated (boolean)\n",
      "\n",
      "6. **bins**\n",
      "   - bin_id (integer)\n",
      "   - bin_name (varchar)\n",
      "\n",
      "7. **phases**\n",
      "   - phase_id (integer)\n",
      "   - phase_name (varchar)\n",
      "   - phase_stage (varchar)\n",
      "\n",
      "8. **endpoints**\n",
      "   - endpoint_id (integer\n"
     ]
    }
   ],
   "source": [
    "#print(response.text) \n",
    "print(complete_response_ZF.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c77aa49b-3c98-4327-9938-f70a2e7ad048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/gradio/components/chatbot.py:222: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7880\n",
      "* Running on public URL: https://93f04755c61d4af684.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://93f04755c61d4af684.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "#import ollama\n",
    "\n",
    "def format_history(msg: str, history: list[list[str, str]], system_prompt: str):\n",
    "    chat_history = [{\"role\": \"system\", \"content\":system_prompt}]\n",
    "    for query, response in history:\n",
    "        chat_history.append({\"role\": \"user\", \"content\": query})\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": response})  \n",
    "    chat_history.append({\"role\": \"user\", \"content\": msg})\n",
    "    return chat_history\n",
    "\n",
    "def init_loadimages():\n",
    "     #file_path =  \"./jupyter_apps/LLM/imgs/ZF_LDTT.png\" \n",
    "    file_path =  \"./imgs/ZF_LDTT.png\" \n",
    "    \n",
    "    try:\n",
    "        # Read the uploaded image file\n",
    "        with open(file_path, \"rb\") as img_file:\n",
    "            image_data = img_file.read()\n",
    "        \n",
    "        # Convert image to base64 and create ImageDocument\n",
    "        base64str = base64.b64encode(image_data).decode(\"utf-8\")\n",
    "        image_document = ImageDocument(image=base64str, image_mimetype=\"image/png\")\n",
    "        return image_document\n",
    "         \n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "    \n",
    "def generate_response(msg: str, history: list[list[str, str]], system_prompt: str):\n",
    "    chat_history = format_history(msg, history, system_prompt)\n",
    "    image_document = init_loadimages()\n",
    "    response = azure_openai_mm_llm.complete(\n",
    "        prompt=\"Describe the images as an alternative text\",\n",
    "        image_documents=[image_document],\n",
    "    )\n",
    "      # Check response type and print it\n",
    "    print(\"Response type:\", type(response))\n",
    "    \n",
    "    # Handle CompletionResponse\n",
    "    if isinstance(response, llama_index.legacy.core.llms.types.CompletionResponse):\n",
    "        try:\n",
    "            content = response.text  # Assuming the content is in the \"text\" attribute\n",
    "            content = \"\"\"{ 'role': 'assistant',\n",
    "    'content': 'The image is a database schema diagram with several tables, including sources, chemicals, plates, well_types}\"\"\" \n",
    "            content =\"\"\"{'chat_message':'hello'}\"\"\"\n",
    "            \n",
    "            content = {'role': 'assistant','content': 'HHHHHHHHello, how can I assist you today?'}\n",
    "            \n",
    "\n",
    "            yield {\"role\": \"assistant\", \"content\": content}\n",
    "        except AttributeError:\n",
    "            # Handle potential absence of \"text\" attribute\n",
    "            yield {\"role\": \"assistant\", \"content\": \"Error: Unexpected response format\"}\n",
    "    else:\n",
    "        # Handle other unexpected formats (if needed)\n",
    "        yield {\"role\": \"assistant\", \"content\": \"Error: Unexpected response format\"}\n",
    "    \n",
    "    \n",
    "    # Check if response is a string (assuming it's the desired format)\n",
    "    if isinstance(response, str):\n",
    "        # Yield the complete response as a single message\n",
    "        yield {\"role\": \"assistant\", \"content\": response}\n",
    "        \n",
    "    else:\n",
    "        if isinstance(response, list):\n",
    "          # Handle list format (assuming first element is content)\n",
    "            try:\n",
    "                content = response[0]  # Assuming content is at index 0 (adjust if needed)\n",
    "                content = {'role': 'assistant','content': 'Hello, how can I assist you today?'}\n",
    "                if content:\n",
    "                    yield {\"role\": \"assistant\", \"content\": content}\n",
    "            except IndexError:\n",
    "                # Handle potential issues with accessing list elements\n",
    "                yield {\"role\": \"assistant\", \"content\": \"Error: Unexpected response format\"}\n",
    "        else:\n",
    "            # Handle other unexpected formats (if needed)\n",
    "            yield {\"role\": \"assistant\", \"content\": \"Error: Unexpected response format 1 \"}\n",
    "        # Handle other response formats (if necessary)\n",
    "        yield {\"role\": \"assistant\", \"content\": \"Error: Unexpected response format 2 \"}\n",
    "     \n",
    "    #message = response\n",
    "    #print (response)\n",
    "    #yield message\n",
    "\n",
    "chatbot = gr.ChatInterface(\n",
    "                generate_response,\n",
    "                chatbot=gr.Chatbot(\n",
    "                        avatar_images=[\"./imgs/user.png\", \"./imgs/Chatbot.jpeg\"],\n",
    "                        height=\"64vh\"\n",
    "                    ),\n",
    "                additional_inputs=[\n",
    "                    gr.Textbox(\n",
    "                        \"Behave as if you are professional writer.\",\n",
    "                        label=\"System Prompt\"\n",
    "                    )\n",
    "                ],\n",
    "                title=\"LLama-2 (7B) Chatbot using 'Ollama'\",\n",
    "                description=\"Feel free to ask any question.\",\n",
    "                theme=\"soft\",\n",
    "                submit_btn=\"⬅ Send\",\n",
    "                #retry_btn=\"🔄 Regenerate Response\",\n",
    "                #undo_btn=\"↩ Delete Previous\",\n",
    "                #clear_btn=\"🗑️ Clear Chat\"\n",
    ")\n",
    "chatbot.launch( share=True)\n",
    "#chatbot.launch(server_name='https://jupyter-gpu.niehs.nih.gov/', server_port=7860)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df859762-412b-4c9b-bce9-68345fade26f",
   "metadata": {},
   "source": [
    "## Chatbot for the sql images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53fa8c31-1857-40d8-a14d-f04362cb7fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/gradio/components/chatbot.py:222: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7871\n",
      "* Running on public URL: https://4889a6d9b344eb88df.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://4889a6d9b344eb88df.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/gradio/queueing.py\", line 622, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/gradio/blocks.py\", line 2013, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/gradio/blocks.py\", line 1578, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/gradio/utils.py\", line 691, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/gradio/utils.py\", line 796, in asyncgen_wrapper\n",
      "    response = await iterator.__anext__()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/gradio/chat_interface.py\", line 666, in _stream_fn\n",
      "    first_response = await async_iteration(generator)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/gradio/utils.py\", line 691, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/gradio/utils.py\", line 685, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2405, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 914, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ddn/gs1/home/noltesz/.conda/envs/py311/lib/python3.11/site-packages/gradio/utils.py\", line 668, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2239099/4079005690.py\", line 42, in generate_response\n",
      "    if isinstance(partial_resp[\"message\"], dict):\n",
      "                  ~~~~~~~~~~~~^^^^^^^^^^^\n",
      "TypeError: tuple indices must be integers or slices, not str\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "#import ollama\n",
    "\n",
    "def format_history(msg: str, history: list[list[str, str]], system_prompt: str):\n",
    "    chat_history = [{\"role\": \"system\", \"content\":system_prompt}]\n",
    "    for query, response in history:\n",
    "        chat_history.append({\"role\": \"user\", \"content\": query})\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": response})  \n",
    "    chat_history.append({\"role\": \"user\", \"content\": msg})\n",
    "    return chat_history\n",
    "\n",
    "def init_loadimages():\n",
    "     #file_path =  \"./jupyter_apps/LLM/imgs/ZF_LDTT.png\" \n",
    "    file_path =  \"./imgs/ZF_LDTT.png\" \n",
    "    \n",
    "    try:\n",
    "        # Read the uploaded image file\n",
    "        with open(file_path, \"rb\") as img_file:\n",
    "            image_data = img_file.read()\n",
    "        \n",
    "        # Convert image to base64 and create ImageDocument\n",
    "        base64str = base64.b64encode(image_data).decode(\"utf-8\")\n",
    "        image_document = ImageDocument(image=base64str, image_mimetype=\"image/png\")\n",
    "        return image_document\n",
    "         \n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "    \n",
    "def generate_response(msg: str, history: list[list[str, str]], system_prompt: str):\n",
    "    chat_history = format_history(msg, history, system_prompt)\n",
    "    image_document = init_loadimages()\n",
    "    response = azure_openai_mm_llm.complete(\n",
    "        prompt=\"Describe the images as an alternative text\",\n",
    "        image_documents=[image_document],\n",
    "    )\n",
    "    message = response\n",
    "    #print (response)\n",
    "    for partial_resp in response:\n",
    "        try:\n",
    "            # Check if \"message\" is a dictionary and access content\n",
    "            if isinstance(partial_resp[\"message\"], dict):\n",
    "                token = partial_resp[\"message\"][\"content\"]\n",
    "            # Handle list format (example)\n",
    "            elif isinstance(partial_resp[\"message\"], list):\n",
    "                for message_item in partial_resp[\"message\"]:\n",
    "                    content = message_item.get(\"content\", None)\n",
    "                    if content:\n",
    "                        message += content\n",
    "                        break  # Stop iterating after finding content (optional)\n",
    "            elif isinstance(partial_resp[\"message\", tuple]):\n",
    "                content = partial_resp[\"message\"][0]\n",
    "                if content:\n",
    "                  message += content\n",
    "            else:\n",
    "                # Handle other formats if needed\n",
    "                pass\n",
    "            message += token\n",
    "        except KeyError:\n",
    "            # Handle potential absence of \"message\" key\n",
    "            message += \"Error: Could not access response content.\"\n",
    "        yield message\n",
    "\n",
    "chatbot = gr.ChatInterface(\n",
    "                generate_response,\n",
    "                chatbot=gr.Chatbot(\n",
    "                        avatar_images=[\"./imgs/user.png\", \"./imgs/Chatbot.jpg\"],\n",
    "                        height=\"64vh\"\n",
    "                    ),\n",
    "                additional_inputs=[\n",
    "                    gr.Textbox(\n",
    "                        \"Behave as if you are professional writer.\",\n",
    "                        label=\"System Prompt\"\n",
    "                    )\n",
    "                ],\n",
    "                title=\"LLama-2 (7B) Chatbot using 'Ollama'\",\n",
    "                description=\"Feel free to ask any question.\",\n",
    "                theme=\"soft\",\n",
    "                submit_btn=\"⬅ Send\",\n",
    "                #retry_btn=\"🔄 Regenerate Response\",\n",
    "                #undo_btn=\"↩ Delete Previous\",\n",
    "                #clear_btn=\"🗑️ Clear Chat\"\n",
    ")\n",
    "chatbot.launch( share=True)\n",
    "#chatbot.launch(server_name='https://jupyter-gpu.niehs.nih.gov/', server_port=7860)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39cf2e1-bf5e-4f1f-b66e-a1d9400c4de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c80f9a-0652-4bf8-b94b-7d1d696fa75d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py38)",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
